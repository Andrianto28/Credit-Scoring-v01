# -*- coding: utf-8 -*-
"""FraudIndividualUser_Fix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AF-X5Tei18pNFMVTNnIufNajZsLOd8H8
"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# import dataset 
df = pd.read_csv(r'D:\Magang M-Knows\dummy_data.csv')
df.head(10)

df_vis = df.copy()

#transform data
df_vis['purchasehistory'] = df_vis['purchasehistory'].replace({0: 'Phone', 1: 'Luxury Bag', 2: 'Watch', 3: 'clothes' })
df_vis['loc'] = df_vis['loc'].replace({0: 'flats', 1: 'Apartments', 2:'Elite Housing', 3:'Rent House'})
df_vis['deviceinformation'] = df_vis['deviceinformation'].replace({0: 'iPhone', 1: 'iPad', 2: 'MacBook', 3: 'iMac', 4: 'Samsung', 5: 'Ovo', 6: 'Vivo'})
df_vis['timeofday'] = df_vis['timeofday'].replace({0: 'Morning', 1: 'Afternoon', 2: 'Evening', 3: 'Night'})
df_vis['transactionamount'] = df_vis['transactionamount'].replace({0: '<5 jt', 1: '5-10 jt', 2: '10-15 jt', 3: '>15 jt'})
df_vis['paymentmethod'] = df_vis['paymentmethod'].replace({0: 'Debit Card', 1: 'Credit Card', 2: 'Prepaid Card', 3: 'Bank Transfers', 4: 'Cash'})
df_vis['velocity'] = df_vis['velocity'].replace({0: 'Low', 1: 'Medium', 2: 'High'})
df_vis['sessionduration'] = df_vis['sessionduration'].replace({0: '<5 hours', 1: '5-10 hours', 2: '10-15 hours', 3: '>15 hours'})
df_vis['behavioralbio'] = df_vis['behavioralbio'].replace({0: 'Bad', 1: 'Nomal', 2: 'Good', 3: 'Great'})
df_vis['commhistory'] = df_vis['commhistory'].replace({0: 'Once', 1: 'Seldom', 2:'Often'})
df_vis['socialmediahistory'] = df_vis['socialmediahistory'].replace({0: 'Passive', 1: 'Active'})
df_vis['priority'] = df_vis['priority'].replace({0: 'No', 1: 'Yes'})
df_vis['target'] = df_vis['target'].replace({0: 'No Fraud', 1: 'Is Fraud'})

df_vis.head()

# df_vis.to_csv('datafraudindividual.csv', index=False)

df['commhistory'] = df['commhistory'].astype(int)
df['purchasehistory'] = df['purchasehistory'].astype(int)
df['loc'] = df['loc'].astype(int)
df['timeofday'] = df['timeofday'].astype(int)
df['deviceinformation'] = df['deviceinformation'].astype(int)
df['transactionamount'] = df['transactionamount'].astype(int)
df['paymentmethod'] = df['paymentmethod'].astype(int)
df['velocity'] = df['velocity'].astype(int)
df['sessionduration'] = df['sessionduration'].astype(int)
df['behavioralbio'] = df['behavioralbio'].astype(int)
df['socialmediahistory'] = df['socialmediahistory'].astype(int)
df['priority'] = df['priority'].astype(int)
df['target'] = df['target'].astype(int)

df2 = pd.DataFrame({'deviceinformation': df['deviceinformation'], 'transactionamount': df['transactionamount'], 'paymentmethod': df['paymentmethod'], 'velocity': df['velocity'], 'sessionduration': df['sessionduration'], 'behavioralbio': df['behavioralbio'], 'socialmediahistory': df['socialmediahistory'], 'priority': df['priority'], 'loc' : df['loc'], 'purchasehistory' : df['purchasehistory'], 'timeofday' : df['timeofday'], 'commhistory' : df['commhistory'] , 'target' : df['target']})
df2

"""EXPLORATORY DATA ANALYSIS (EDA)"""

df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.shape

unique_values = {}
for col in df_vis.columns:
    unique_values[col] = df_vis[col].value_counts().shape[0]

pd.DataFrame(unique_values, index=['unique value count']).transpose()

import matplotlib.pyplot as plt
df.plot(kind="box", subplots=True, layout=(7,4), figsize=(15,14));

#Uji Korelasi

correlation = df2.corr(method='pearson')
correlation

import seaborn as sns
sns.heatmap(correlation,xticklabels=correlation.columns,
            yticklabels=correlation.columns)

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='deviceinformation')
plt.xticks(rotation=90)
plt.title('Target by device information',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='transactionamount')
plt.xticks(rotation=90)
plt.title('Target by transaction amount',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='commhistory')
plt.xticks(rotation=90)
plt.title('Target by communication history',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='loc')
plt.xticks(rotation=90)
plt.title('Target by location',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='timeofday')
plt.xticks(rotation=90)
plt.title('Target by time of day',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='sessionduration')
plt.xticks(rotation=90)
plt.title('Target by session duration',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='purchasehistory')
plt.xticks(rotation=90)
plt.title('Target by purchase history',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='paymentmethod')
plt.xticks(rotation=90)
plt.title('Target by payment method',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='velocity')
plt.xticks(rotation=90)
plt.title('Target by velocity',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='behavioralbio')
plt.xticks(rotation=90)
plt.title('Target by behavioralbio',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='socialmediahistory')
plt.xticks(rotation=90)
plt.title('Target by social media history',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

#Visualisasi Interaktif
plt.figure(figsize=(12,4))
x= sns.countplot(x='target',data=df_vis,hue='priority')
plt.xticks(rotation=90)
plt.title('Target by priority',fontdict={'fontsize':20})
for i in x.patches:
    x.annotate('{:.2f}'.format((i.get_height()/df_vis.shape[0])*100)+'%',(i.get_x()+0.25, i.get_height()+0.01))
plt.show()

"""UJI ASUMSI

Uji Normalitas
Jika sampel (n) < 50 gunakan shapiro wilk, tetapi jika n > 50 gunakan kolmogorov smirnov

Ho : Data berdistribusi normal
H1 : Data tidak berdistribusi normal

Kriteria uji: Tolak Ho jika pvalue < 0.05 terima dalam hal lainnya
"""

#Uji Asumsi Normalitas
import numpy as np
from scipy.stats import kstest

# Loop over columns of df2 and perform KS test on each
for col in df2.columns:
    test_statistic, p_value = kstest(df2[col], 'norm')
    print(f"Column: {col}")
    print("Test statistic:", test_statistic)
    print("P-value:", p_value)

"""Karena masing-masing variabel memiliki p-value < 0.05 maka data tidak normal dan perlu dilakukan trasnformasi data"""

# Transformasi Data 
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
segmentation_std = scaler.fit_transform(df2)

"""Uji Homogenitas Varians

Untuk menghitung nilai eigen dan vektor eigen
Jika data memenuhi asumsi homogenitas varians, gunakan matriks varkov,
jika tidak terpenuhi gunakan matriks korelasi.

Ho : Matriks varians kovarians homogen
H1 : Matriks varians kovarians heterogen

Kriteria Uji: Tolak Ho jika pvalue < 0.05 dan terima dalam hal lainnya.
"""

# Uji Homogenitas Varians
from scipy.stats import bartlett

# convert each column of df2 to a one-dimensional array
data = [df2[col].values for col in df2.columns]

bartlett_test_statistic, p_value = bartlett(*data)
print("Test statistic:", bartlett_test_statistic)
print("P-value:", p_value)

# Karena pvalue < 0.05 maka matriks varians heterogen (tidak memenuhi asumsi homogenitas)

"""Split train (80%) and test data (20%)"""

from sklearn.model_selection import train_test_split

y = df2['target']
x = df2.drop(['target'], axis=1)
x_train , x_test , y_train , y_test = train_test_split(x,y , test_size=0.20, random_state=44, shuffle =True)

# Print the shapes of the training and testing sets
print("Training data shape: ", x_train.shape)
print("Testing data shape: ", x_test.shape)

"""PRINCIPAL COMPONENT ANALYSIS

Split data jadi train dan test yang udah di transformasi (data sudah berdistribusi normal)
"""

# Transformasi Data 
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

#PCA
from sklearn.decomposition import PCA
pca =PCA()
pca.fit(x_train_scaled)

# Calculate the explained variance by components
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

pca.explained_variance_ratio_

# Plot the cumulative explained variance
plt.figure(figsize=(10,8))
plt.plot(range(1,len(cumulative_variance_ratio)+1), cumulative_variance_ratio, marker='o', linestyle='--')
plt.title('Explained Variance by Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

print("Cumulative explained variance ratio:", cumulative_variance_ratio)

"""Berdasarkan hasil plot dan cumulative explained variance, dapat dilihat bahwa cumulative proportion (proporsi varians kumulatif) dari komponen utama pertama dapat menjelaskan 9% total varians, dan bila ditambahkan dengan komponen utama kesepeluh maka akan menjadi 85.90%. Artinya, apabila peneliti mengambil 10 komponen yaitu Komponen Utama Pertama, Kedua sampai kesepuluh maka sudah mencukupi karena nilai cumulative proportion nya sudah melebihi 80%. Sehingga, model persamaan variabel dependen terhadap komponen utama adalah 10 komponen."""

#We choose three component (>80%)
pca = PCA(n_components=10)

#Fit the model
X_train_pca = pca.fit_transform(x_train_scaled)
X_test_pca = pca.transform(x_test_scaled)
scores_pca = pca.transform(x_train_scaled)

pca_test = pca.transform(x_test_scaled)

hasil_pca = pd.DataFrame(pca_test)
hasil_pca

"""evaluate the performance of the PCA model using cross-validation. """

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# create a logistic regression model
model = LogisticRegression()

# evaluate the performance using cross-validation
scores = cross_val_score(model, X_test_pca, y_test, cv=5)

# print the average score across all folds
print("Average cross-validation score:", np.mean(scores))

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# create a logistic regression model
model = LogisticRegression()

# evaluate the performance using cross-validation
scores = cross_val_score(model, X_train_pca, y_train, cv=5)

# print the average score across all folds
print("Average cross-validation score:", np.mean(scores))

"""K MEANS DENGAN PCA"""

# K-means clustering with PCA
from sklearn.cluster import KMeans
# We fit K Means using the transformed data from the PCA
wcss = []
for i in range(1,10):
    kmeans_pca = KMeans(n_clusters=i, init= 'k-means++', random_state=42)
    kmeans_pca.fit(scores_pca)
    wcss.append(kmeans_pca.inertia_)

plt.gca()
plt.plot(range(1,10), wcss, marker='o', linestyle= '--')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.title('K-means with PCA clustering')
plt.show()

# We have chosen 2 clusters
# some initializer and random state as before
kmeans_pca = KMeans(n_clusters=2, init = "k-means++", random_state=42)

# fit our data with kmeans pca model
kmeans_pca.fit(scores_pca)

# Kmeans clustering with PCA result
# convert x_train_scaled to a pandas DataFrame
df_train_scaled = pd.DataFrame(x_train_scaled, columns=x_train.columns)

# K-Means with PCA result
# create a new df with the original features and add the PCA scores and assigned clusters
df_segm_pca_kmeans = pd.concat([df_train_scaled, pd.DataFrame(scores_pca)], axis=1)
df_segm_pca_kmeans.columns.values[-10:] = ['Component 1', 'Component 2', 'Component 3','Component 4', 'Component 5', 'Component 6', 'Component 7', 'Component 8', 'Component 9', 'Component 10']
# the last column we add contains the pca k-means clustering labels
df_segm_pca_kmeans['Segment K-means PCA'] = kmeans_pca.labels_

df_segm_pca_kmeans.head()

print(df_segm_pca_kmeans['paymentmethod'].value_counts())

# reverse the scaling using the inverse_transform method
original_x_train = scaler.inverse_transform(df_train_scaled)
df_original_x_train = pd.DataFrame(original_x_train, columns= x_train.columns)
df_original_x_train['paymentmethod'].value_counts()

"""transformasi:

purchase history = -1.285159: Phone, -0.387231: Luxury Bag,  0.510696: Watch,  1.408624: clothes
loc = -1.354518: 'flats', -0.453758: apartments,  0.447002: elite house, 3: 1.347762
device information = -1.480996: iPhone, -0.983600: iPad, -0.486204: Macbook, 0.011191: iMac, 0.508587: Samsung, 1.005983: Ovo, 1.503379: Vivo
time of day = -1.348054: Morning, -0.459715: Afternoon,  0.428623: Evening, 1.316962: Night
transaction amount = -1.396260: <5 jt, -0.478423: 5-10 jt, 0.439415: 10-15 jt, 1.357252: >15 jt
payment method = 0: debit card, 1: credit card, 2: prepaid card, 3: bank transfers, 4: cash
velocity = 0: low, 1: medium, 2: high
sessionduration = 0: <5 hours, 1: 5-10 hours, 2: 10-15 hours, 3: >15 hours
behavioralbio = 0: bad, 1: normal, 2: good, 3: great
commhistory = 0: once, 1: seldom, 2: often
socialmediahistory = 0: passive, 1: active
priority= 0: no, 1: yes

baseline:

purchase history = 0: Phone, 1: Luxury Bag, 2: Watch, 3: clothes
loc = 0: 'flats', 1: apartments, 2: elite house, 3: rent house
device information = 0: iPhone, 1: iPad, 2: Macbook, 3: iMac, 4: Samsung, 5: Ovo, 6: Vivo
time of day = 0: Morning, 1: Afternoon, 2: Evening, 3: Night
transaction amount = 0: <5 jt, 1: 5-10 jt, 2: 10-15 jt, 3: >15 jt
payment method = 0: debit card, 1: credit card, 2: prepaid card, 3: bank transfers, 4: cash
velocity = 0: low, 1: medium, 2: high
sessionduration = 0: <5 hours, 1: 5-10 hours, 2: 10-15 hours, 3: >15 hours
behavioralbio = 0: bad, 1: normal, 2: good, 3: great
commhistory = 0: once, 1: seldom, 2: often
socialmediahistory = 0: passive, 1: active
priority= 0: no, 1: yes
"""

# separate the two segments based on their cluster labels
clust_1 = df_segm_pca_kmeans[df_segm_pca_kmeans['Segment K-means PCA'] == 0]
clust_2 = df_segm_pca_kmeans[df_segm_pca_kmeans['Segment K-means PCA'] == 1]

"""Profiling masing-masing atribut untuk mengetahui karakteristik klaster"""

# describe the statistics for each segment
print("cluster pertama:")
print(clust_1['deviceinformation'].value_counts())

print("Cluster Kedua:")
print(clust_2['deviceinformation'].value_counts())

# # add the names of the segments to the labels
df_segm_pca_kmeans['Segment'] = df_segm_pca_kmeans['Segment K-means PCA'].map({0:'No Fraud', 1: 'Is Fraud'})

# plot data by PCA Component, the Y axis is the first compnent, X axis is the second Component.
x_axis = df_segm_pca_kmeans['Component 2']
y_axis = df_segm_pca_kmeans['Component 1']
plt.figure(figsize=(10,8))
sns.scatterplot(x=x_axis, y=y_axis, hue=df_segm_pca_kmeans['Segment'], palette=['g','r'])
plt.title('Clusters by PCA Component')
plt.show()

# plot data by PCA Component, the Y axis is the first compnent, X axis is the second Component.
x_axis = df_segm_pca_kmeans['Component 10']
y_axis = df_segm_pca_kmeans['Component 9']
plt.figure(figsize=(10,8))
sns.scatterplot(x=x_axis, y=y_axis, hue=df_segm_pca_kmeans['Segment'], palette=['g','r'])
plt.title('Clusters by PCA Component')
plt.show()

""" performing k-means clustering with PCA using cross-validation"""

from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# Create a pipeline for PCA and k-means clustering
pipeline = Pipeline([
    ('pca', pca),
    ('kmeans', kmeans_pca)
])

# Use cross-validation to evaluate the pipeline
scores = cross_val_score(pipeline, x_train_scaled, cv=5)

# Print the mean and standard deviation of the scores
print("Mean score:", scores.mean())
print("Standard deviation:", scores.std())

"""Skor rata-rata dan deviasi standar dalam validasi silang memberikan perkiraan kinerja model pada data yang tidak terlihat. Dalam hal ini, skor rata-rata -1762,56 menunjukkan bahwa model pengelompokan K-means dengan PCA mampu mengelompokkan data dengan cukup baik. Nilai negatif menunjukkan bahwa model lebih baik daripada baseline, yang sering terjadi pada model pembelajaran tanpa pengawasan. Standar deviasi 21,08 menunjukkan bahwa kinerja model konsisten di seluruh lipatan yang berbeda dalam validasi silang, dan memberikan ukuran variabilitas estimasi kinerja. Standar deviasi yang rendah menunjukkan bahwa kinerja model stabil, sedangkan standar deviasi yang tinggi menunjukkan bahwa kinerja mungkin lebih bervariasi di seluruh subset data yang berbeda."""